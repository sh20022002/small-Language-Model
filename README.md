# ðŸ§  `my_slm` â€” tiny SLM + Hybrid Tokenizer (Colabâ€‘friendly)

This project contains a **hybrid tokenizer** and a small Transformer training loop you can run in Colab.  
The tokenizer mixes classic subword ideas with a **UTFâ€‘8 byte fallback** so it can encode *any* text, then exposes both **RLE** and **flat** output modes â€” perfect for demos and model training.  
The SLM bits include a minimal Transformer, dataloader, and training utilities.

> Tokenizer highlights (from the original design): encodes arbitrary Unicode, prefers short common words as base tokens, supports greedy merges, and falls back to raw bytes when needed. It can return **runâ€‘length pairs** or a **flat id list**.


## Repo layout

```
your-repo/
â”œâ”€ setup.py
â”œâ”€ README.md
â”œâ”€ notebooks/                        #  (not packaged)
â”œâ”€ tests/                
   â””â”€mfu.py                          # tests device flops
â””â”€ src/
   â””â”€ my_slm/                        # installable package
      â”œâ”€ __init__.py
      â”œâ”€ hybrid_tokeniztion.py       # HybridTokenizer
      â”œâ”€ transformer.py              # tiny Transformer
      â”œâ”€ train.py                    # dataloader + training loop
      â”œâ”€ multi_train_orchestrator.py # trains on datasets
      â”œâ”€ benchmark_logger.py         # tests model on benchmark and logs reoults
      â””â”€ data/
         â””â”€ tokenizer_state.pkl.gz   # copressed freezed tokenizer
```

---

## Installation

### From GitHub (nonâ€‘editable)
```bash
pip install "git+https://github.com/sh20022002/small-Language-Model.git@main"
```

### Editable dev install
```bash
git clone https://github.com/sh20022002/small-Language-Model/tree/main.git
pip install -e ./small-Language-Model
```

> Packaging uses a `src/` layout with the import package **`my_slm`**.


---

## Quickstart â€” Hybrid Tokenizer demo (keep this section in the README âœ…)

```python
from my_slm.hybrid_tokeniztion import HybridTokenizer

# 1) Build the frequency DB
tok = HybridTokenizer()
tok.add_text("Hello world, welcome to tokenization.")
# or: tok.add_file("path/to/text.txt"); tok.add_files("data/**/*.txt")

# 2) Freeze vocab (choose how many base tokens and merges to keep)
tok.freeze_vocab(k_bases=5000, max_merges=10000)

# 3) Inspect DB status
tok.db_status(preview=10)

# 4) Encode/Decode in both modes
s = "Hello, world!\n×©×œ×•×  ðŸ™‚"
enc_rle  = tok.encode(s, mode="rle")   # -> list[(token_id, count)]
enc_flat = tok.encode(s, mode="flat")  # -> list[int]
assert tok.decode(enc_rle)  == s
assert tok.decode(enc_flat) == s

# 5) Explain / segment (nice for demos)
tok.top_merges(10)              # see learned merges
parts = tok.explain_token("welcome")   # recursive decomposition
seg   = tok.segment("Hello ×©×œ×•×")      # token kinds: base/merge/byte/sp/nl
print(parts, seg)
```

**Why flat vs RLE?**  
- Use **`mode="rle"`** for compact, humanâ€‘readable dumps and roundâ€‘trip tests.  
- Use **`mode="flat"`** for **model training** (IDs shape `[B, T]`).

> Design notes from the tokenizer source: RLE or flat outputs, greedy longestâ€‘match encoding, UTFâ€‘8 fallback, and vocabulary freezing before encode/decode.


---

## Train the tiny SLM

```python
import torch
from torch.optim import AdamW
from my_slm.transformer import Transformer
from my_slm.hybrid_tokeniztion import HybridTokenizer

# 1) Build + freeze tokenizer
tok = HybridTokenizer()
tok.add_files("data/**/*.txt")
tok.freeze_vocab(k_bases=5000, max_merges=10000)
pad_id = tok.token2id["<PAD>"]

# 2) Create model and sync vocab BEFORE moving to device
model = Transformer(...)
model.resize_token_embeddings(tok.vocab_size)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
opt = AdamW(model.parameters(), lr=3e-4)

# 3) Dataloader should yield:
#    - input_ids: LongTensor [B, T] (built with tok.encode(text, mode='flat'))
#    - attention_mask: [B, T] bool/long (ids!=pad_id)
#    - labels: [B, T] with pad positions == pad_id
#    CrossEntropyLoss(ignore_index=pad_id)

# 4) Forward
ids = ids.long().to(device)
mask = (ids != pad_id).to(device)
logits = model(ids, attention_mask=mask)  # [B, T, V]
```

**Training tips**
- Keep token IDs `int64`; you can run model compute in `bf16/fp16` via autocast.
- If you ever grow the tokenizer after model init, **resize** the embeddings **and** the output head to match the new vocab.
- Labels/padding: set `ignore_index = pad_id` so padding doesnâ€™t contribute to loss.


---

## Colab usage

```python
# simple install
%pip install -q "git+https://github.com/sh20022002/small-Language-Model/tree/main.git@main"

# or live-edit workflow
!git clone https://github.com/sh20022002/small-Language-Model/tree/main.git
%pip install -e /content/small-Language-Model
```

Tips:
- Donâ€™t put notebooks inside `src/` â€” keep them under `notebooks/`.
- If you don't see `setup.py` in Colab after a nonâ€‘editable install, that's expected (pip installs the built wheel).


---

## Tests
```bash
pytest -q
```

---

## Troubleshooting

- **Unexpected kwarg `attention_mask`** â†’ ensure `Transformer.forward(self, x, attention_mask=None)` accepts (can ignore) the mask.
- **ModuleList has no forward** â†’ iterate blocks: `for block in self.blocks: x = block(x)`.
- **Vocab/label mismatch** â†’ align `tokenizer.vocab_size == embedding.num_embeddings == head.out_features` and set `ignore_index=pad_id`.
- **Device mismatch** â†’ if you resize layers after `model.to(device)`, move to device again and recreate the optimizer.
- **Batch length mismatch** â†’ make labels from the **padded ids** and flatten `(B*T)` for both logits and labels before `CrossEntropyLoss`.
- **Autograd warning adding loss** â†’ use `loss.detach().item()` for running sums.

---

## License
MIT

